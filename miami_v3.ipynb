{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5879130d-4b42-4952-9249-5299ab6d2513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from multimodal_model import MMLSTM, BenchmarkLSTM\n",
    "device =  'cuda'\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0e9fa-0772-4afc-8462-89aa0a6c4b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc4c20-1e32-4d44-8fb3-e25411baff9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./data/miami_with_tag/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71921b5d-e9e8-4f94-9657-91e02c04e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# assign directory\n",
    "directory = DATA_PATH + 'eng/'\n",
    "\n",
    "def get_miami_data(directory, train_pct=0.8):\n",
    "    eng_dfs = None\n",
    "    # iterate over files in\n",
    "    # that directory\n",
    "    for filename in os.listdir(directory):\n",
    "        f = os.path.join(directory, filename)\n",
    "        # checking if it is a file\n",
    "        if os.path.isfile(f):\n",
    "            if eng_dfs is None:\n",
    "                eng_dfs = pd.read_csv(f)\n",
    "            else:\n",
    "                next_df = pd.read_csv(f)\n",
    "                eng_dfs = pd.concat([eng_dfs, next_df], ignore_index=True,axis=0)\n",
    "\n",
    "    filtered_eng_dfs = eng_dfs[eng_dfs['sentence'].apply(lambda x: len(x.split()) > 3)].reset_index()\n",
    "\n",
    "    import regex as reg\n",
    "    filtered_eng_dfs.sentence = filtered_eng_dfs.sentence.apply(lambda x: x.replace(r'/', ''))\n",
    "\n",
    "    all_eng = ' '.join(filtered_eng_dfs.sentence)\n",
    "    from multi_bpe import MultiBPE\n",
    "    multi_bpe = MultiBPE()\n",
    "\n",
    "    all_tokens = []\n",
    "    for i in range(len(filtered_eng_dfs.sentence)):\n",
    "        all_tokens.extend(multi_bpe.encode(filtered_eng_dfs.sentence[i],\n",
    "                                           padding=False,\n",
    "                                           use_eos=False))\n",
    "\n",
    "    train_tokens, val_tokens = all_tokens[:int(len(all_tokens)*train_pct)], all_tokens[int(len(all_tokens)*train_pct):]\n",
    "    return train_tokens, val_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8884b2a-5ae8-4c11-bf24-78ba2e10a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_train_tokens, eng_val_tokens = get_miami_data(DATA_PATH + 'eng/')\n",
    "spn_train_tokens, spn_val_tokens = get_miami_data(DATA_PATH + 'spa/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97590b4f-6e02-422a-9c37-dea32b890271",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_tokens = [tok for tok in eng_train_tokens]\n",
    "all_train_tokens.extend(spn_train_tokens)\n",
    "all_val_tokens = [tok for tok in spn_val_tokens]\n",
    "all_val_tokens.extend(spn_val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75f285-b200-4219-a5f4-599c6908ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_train_tokens) == len(eng_train_tokens) + len(spn_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f047afb-b471-4dd9-944e-72a301adf2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_img(image_path): \n",
    "    transform = torchvision.transforms.Compose([\n",
    "        # Resize image to 224 x 224 as required by most vision models\n",
    "        torchvision.transforms.Resize(\n",
    "            size=(224, 224)\n",
    "        ),\n",
    "        # Convert PIL image to tensor with image values in [0, 1]\n",
    "        torchvision.transforms.ToTensor(),\n",
    "\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    im = PIL.Image.open(image_path)\n",
    "    image = im.convert('RGB')\n",
    "    image = transform(image)\n",
    "    \n",
    "    return image.view(1, image.size(0), image.size(1), image.size(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db616d67-0788-4cb8-889e-394783935c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuning(loaded_model, \n",
    "               lr=5e-4, \n",
    "               is_multimodal=False,\n",
    "               train_visual_module=False,\n",
    "               is_benchmark=False,\n",
    "               herring_train_text=all_train_tokens,\n",
    "               herring_test_text=all_val_tokens,\n",
    "               max_length = 64,\n",
    "               stride = 32,\n",
    "               epochs = 5):\n",
    "    \n",
    "    herring_ids = herring_train_text\n",
    "    \n",
    "    herring_test_ids = herring_test_text\n",
    "    \n",
    "    if is_benchmark:\n",
    "        mm_model = BenchmarkLSTM().to(device)\n",
    "    else:\n",
    "        mm_model = MMLSTM(is_multimodal=is_multimodal,\n",
    "                     train_visual_module=train_visual_module).to(device)\n",
    "    \n",
    "    mm_model.load_state_dict(torch.load(f'./saved_models/{loaded_model}'))\n",
    "    mm_model.train()\n",
    "\n",
    "    inp = torch.tensor(herring_ids).view(1,-1)\n",
    "    test_inp = torch.tensor(herring_test_ids).view(1,-1)\n",
    "\n",
    "    white_img_path = './images/white_img.png'\n",
    "\n",
    "    optimizer = torch.optim.Adam(mm_model.parameters(), lr=lr)\n",
    "    loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        nlls = []\n",
    "        for i in tqdm(range(0, inp.size(1), stride)):\n",
    "            begin_loc = max(i + stride - max_length, 0)\n",
    "            end_loc = min(i + stride, inp.size(1))\n",
    "            trg_len = end_loc - i  # may be different from stride on last loop\n",
    "            input_ids = inp[:, begin_loc:end_loc].to(device)\n",
    "            target_ids = input_ids.clone().to(device)\n",
    "            target_ids[:, :-trg_len] = -100\n",
    "\n",
    "            input_ids = input_ids[..., :-1].contiguous()\n",
    "            target_ids = target_ids[..., 1:].contiguous()\n",
    "\n",
    "            img = process_img(white_img_path).to(device)\n",
    "            if is_benchmark:\n",
    "                 output = mm_model(input_ids)\n",
    "            else:\n",
    "                output = mm_model.forward_text(input_ids)\n",
    "            mm_model.zero_grad()\n",
    "            loss = torch.nn.functional.cross_entropy(output.view(-1, output.size(-1)), target_ids.view(-1))\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(mm_model.parameters(), 2.0)\n",
    "            optimizer.step()\n",
    "            neg_log_likelihood = loss * trg_len\n",
    "            nlls.append(neg_log_likelihood)\n",
    "\n",
    "            if i % 200 == 1:\n",
    "                print(\"Current train loss:\", epoch_train_loss/num_train_steps)\n",
    "\n",
    "        ppl1 = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "        \n",
    "        print(f'Training Perplexity for epoch {epoch}: {ppl1}')\n",
    "\n",
    "    mm_model.eval()\n",
    "    inp = test_inp\n",
    "    nlls = []\n",
    "    for i in tqdm(range(0, inp.size(1), stride)):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, inp.size(1))\n",
    "        trg_len = end_loc - i  # may be different from stride on last loop\n",
    "        input_ids = inp[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone().to(device)\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        input_ids = input_ids[..., :-1].contiguous()\n",
    "        target_ids = target_ids[..., 1:].contiguous()\n",
    "\n",
    "        img = process_img(white_img_path).to(device)\n",
    "        with torch.no_grad():\n",
    "            if is_benchmark:\n",
    "                 output = mm_model(input_ids)\n",
    "            else:\n",
    "                output = mm_model.forward_text(input_ids)\n",
    "            loss = torch.nn.functional.cross_entropy(output.view(-1, output.size(-1)), target_ids.view(-1))\n",
    "            neg_log_likelihood = loss * trg_len\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    ppl1 = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "    \n",
    "    print(\"Validation Perplexity: \")\n",
    "    print(ppl1)\n",
    "    if is_multimodal:\n",
    "        torch.save(mm_model.state_dict(), \n",
    "                       f'./saved_models/finetuned_multimodal_lstm')\n",
    "    else:\n",
    "        torch.save(mm_model.state_dict(), \n",
    "                       f'./saved_models/finetuned_benchmark_lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715ae82-21e4-4b62-9e71-55fb440acb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretraining(loaded_model, \n",
    "               lr=5e-4, \n",
    "               is_multimodal=False,\n",
    "               train_visual_module=False,\n",
    "               is_benchmark=False,\n",
    "               herring_train_text=all_train_tokens,\n",
    "               herring_test_text=all_val_tokens,\n",
    "               max_length = 64,\n",
    "               stride = 32,\n",
    "               epochs = 5):\n",
    "    \n",
    "    # herring_ids = herring_train_text\n",
    "    \n",
    "    herring_test_ids = herring_test_text\n",
    "    \n",
    "    if is_benchmark:\n",
    "        mm_model = BenchmarkLSTM().to(device)\n",
    "    else:\n",
    "        mm_model = MMLSTM(is_multimodal=is_multimodal,\n",
    "                     train_visual_module=train_visual_module).to(device)\n",
    "    \n",
    "    mm_model.load_state_dict(torch.load(f'./saved_models/{loaded_model}'))\n",
    "    mm_model.eval()\n",
    "\n",
    "    # inp = torch.tensor(herring_ids).view(1,-1)\n",
    "    test_inp = torch.tensor(herring_test_ids).view(1,-1)\n",
    "\n",
    "    white_img_path = './images/white_img.png'\n",
    "\n",
    "#     optimizer = torch.optim.Adam(mm_model.parameters(), lr=lr)\n",
    "#     loss_fct = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         nlls = []\n",
    "#         for i in tqdm(range(0, inp.size(1), stride)):\n",
    "#             begin_loc = max(i + stride - max_length, 0)\n",
    "#             end_loc = min(i + stride, inp.size(1))\n",
    "#             trg_len = end_loc - i  # may be different from stride on last loop\n",
    "#             input_ids = inp[:, begin_loc:end_loc].to(device)\n",
    "#             target_ids = input_ids.clone().to(device)\n",
    "#             target_ids[:, :-trg_len] = -100\n",
    "\n",
    "#             input_ids = input_ids[..., :-1].contiguous()\n",
    "#             target_ids = target_ids[..., 1:].contiguous()\n",
    "\n",
    "#             img = process_img(white_img_path).to(device)\n",
    "#             if is_benchmark:\n",
    "#                  output = mm_model(input_ids)\n",
    "#             else:\n",
    "#                 output = mm_model(input_ids, img)\n",
    "#             mm_model.zero_grad()\n",
    "#             loss = torch.nn.functional.cross_entropy(output.view(-1, output.size(-1)), target_ids.view(-1))\n",
    "#             loss.backward()\n",
    "\n",
    "#             torch.nn.utils.clip_grad_norm_(mm_model.parameters(), 1.0)\n",
    "#             optimizer.step()\n",
    "#             neg_log_likelihood = loss * trg_len\n",
    "#             nlls.append(neg_log_likelihood)\n",
    "\n",
    "#             if i % 200 == 1:\n",
    "#                 print(\"Current train loss:\", epoch_train_loss/num_train_steps)\n",
    "\n",
    "#         ppl1 = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "        \n",
    "#         print(f'Training Perplexity for epoch {epoch}: {ppl1}')\n",
    "\n",
    "\n",
    "    inp = test_inp\n",
    "    nlls = []\n",
    "    for i in tqdm(range(0, inp.size(1), stride)):\n",
    "        begin_loc = max(i + stride - max_length, 0)\n",
    "        end_loc = min(i + stride, inp.size(1))\n",
    "        trg_len = end_loc - i  # may be different from stride on last loop\n",
    "        input_ids = inp[:, begin_loc:end_loc].to(device)\n",
    "        target_ids = input_ids.clone().to(device)\n",
    "        target_ids[:, :-trg_len] = -100\n",
    "\n",
    "        input_ids = input_ids[..., :-1].contiguous()\n",
    "        target_ids = target_ids[..., 1:].contiguous()\n",
    "\n",
    "        img = process_img(white_img_path).to(device)\n",
    "        with torch.no_grad():\n",
    "            if is_benchmark:\n",
    "                 output = mm_model(input_ids)\n",
    "            else:\n",
    "                output = mm_model.forward_text(input_ids)\n",
    "            loss = torch.nn.functional.cross_entropy(output.view(-1, output.size(-1)), target_ids.view(-1))\n",
    "            neg_log_likelihood = loss * trg_len\n",
    "        nlls.append(neg_log_likelihood)\n",
    "\n",
    "    ppl1 = torch.exp(torch.stack(nlls).sum() / end_loc)\n",
    "    \n",
    "    print(\"Validation Perplexity: \")\n",
    "    print(ppl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0646396-49e4-415e-bffb-70c2586cf823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, seed, origin_size, size, temperature=1.0):\n",
    "    \"\"\"\n",
    "    :param model: The complete RNN language model\n",
    "    :param seed: The first few wordas of the sequence to start generating from\n",
    "    :param size: The total size of the sequence to generate\n",
    "    :param temperature: This controls how much we follow the probabilities provided by the network. For t=1.0 we just\n",
    "        sample directly according to the probabilities. Lower temperatures make the high-probability words more likely\n",
    "        (providing more likely, but slightly boring sentences) and higher temperatures make the lower probabilities more\n",
    "        likely (resulting is weirder sentences). For temperature=0.0, the generation is _greedy_, i.e. the word with the\n",
    "        highest probability is always chosen.\n",
    "    :return: A list of integers representing a samples sentence\n",
    "    \"\"\"\n",
    "\n",
    "    ls = seed.shape[0]\n",
    "\n",
    "    tokens = seed.to(device)\n",
    "    \n",
    "    for i in range(origin_size+1, size):\n",
    "        probs = model(tokens[None,:])\n",
    "\n",
    "        # Extract the i-th probability vector and sample an index from it\n",
    "        next_token = sample_logits(probs[0, i-1, :], temperature=temperature)\n",
    "        \n",
    "        tokens[i] = next_token\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20cdd94-c7db-4127-b0d9-7fc19f35a507",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_model= 'multimodal_lstm_200000_1.0_15v6'\n",
    "# text_model = 'monomodal_model_50000_0.00025_6'\n",
    "benchmark_model = 'benchmark_model_200000_1.0_15v7'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0632696a-1108-42cf-aa88-51fa919e6130",
   "metadata": {},
   "source": [
    "## English Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6061f627-31f1-4ad1-812c-8497e450f157",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning(visual_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=True,\n",
    "           herring_train_text=eng_train_tokens,\n",
    "           herring_test_text=eng_val_tokens,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf8b0a5-bfcd-4cd2-b894-290e2f221a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finetuning(text_model, \n",
    "#            lr=`.0, \n",
    "#            is_multimodal=False,\n",
    "#            train_visual_module=False,\n",
    "#            max_length = 32,\n",
    "#            stride = 16,\n",
    "#            epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b9e3f-2216-48d0-8236-181c95a2b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning(benchmark_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=False,\n",
    "           train_visual_module=False,\n",
    "           herring_train_text=eng_train_tokens,\n",
    "           herring_test_text=eng_val_tokens,\n",
    "           is_benchmark=True,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b461d4-74cf-4214-86bd-5e99ecad67a9",
   "metadata": {},
   "source": [
    "## Spanish Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4409f70-b843-449d-a7e3-a6d940111ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning(visual_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=True,\n",
    "           herring_train_text=spn_train_tokens,\n",
    "           herring_test_text=spn_val_tokens,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb622e6c-bac2-4825-a7d6-92ddc81d101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning(benchmark_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=False,\n",
    "           train_visual_module=False,\n",
    "           is_benchmark=True,\n",
    "           herring_train_text=spn_train_tokens,\n",
    "           herring_test_text=spn_val_tokens,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596675fc-da1e-4514-95f5-ac2b0ebaa7c8",
   "metadata": {},
   "source": [
    "## English + Spanish Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211e886-d0ac-4ef5-8ef5-778ac403da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning(visual_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=True,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d614b0-22b1-420a-97bd-74a84a88a554",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuning(benchmark_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=False,\n",
    "           train_visual_module=False,\n",
    "           is_benchmark=True,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe1ce1-2e04-45f7-836d-d5b1e7b33ce7",
   "metadata": {},
   "source": [
    "## English + Spanish Zero-shot Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a796eb-c000-4b44-8115-c5b0d6c8568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining(benchmark_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=False,\n",
    "           train_visual_module=False,\n",
    "           is_benchmark=True,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eed423-4891-4ec3-8a16-b9dc6b9545f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining(visual_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=True,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc526f-0c18-4649-912d-3edb3a60baae",
   "metadata": {},
   "source": [
    "## Spanish Zero-shot Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04401e-aa16-41bb-8ff0-0f912df386af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining(benchmark_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=False,\n",
    "           train_visual_module=False,\n",
    "           herring_test_text=spn_val_tokens,\n",
    "           is_benchmark=True,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ceacc-0217-4a58-a3f6-518fcfb8e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining(visual_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=True,\n",
    "           herring_test_text=spn_val_tokens,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c458813-b3ef-40e7-80c9-29fc14b93fd7",
   "metadata": {},
   "source": [
    "## English Zero-shot Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254aed2-0cce-421b-9031-7dd58a216292",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining(benchmark_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=False,\n",
    "           train_visual_module=False,\n",
    "           herring_test_text=eng_val_tokens,\n",
    "           is_benchmark=True,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a673ba7-7956-424a-b16c-5b25ddce0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretraining(visual_model, \n",
    "           lr=5e-4, \n",
    "           is_multimodal=True,\n",
    "           herring_test_text=eng_val_tokens,\n",
    "           max_length = 32,\n",
    "           stride = 16,\n",
    "           epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88303f-f540-42a7-998d-eccc3d9cba34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d076d78-708f-4ff6-9320-36d7064248ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
