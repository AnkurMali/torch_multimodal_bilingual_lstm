{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkn002/.local/lib/python3.9/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import PIL\n",
    "import math\n",
    "import datetime\n",
    "import os\n",
    "from bpemb import BPEmb\n",
    "from torch import nn\n",
    "from image_caption_dataset import ImageCaptionDataset\n",
    "from multi_bpe import MultiBPE\n",
    "from multimodal_model import MMLSTM, BenchmarkLSTM\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LOAD_PATH = './data/'\n",
    "seed = 420\n",
    "num_data = 200000\n",
    "maxlen = 32\n",
    "epochs = 15\n",
    "train_pct = 0.8\n",
    "bs=32\n",
    "lr = 1.0/math.sqrt(32/bs)\n",
    "is_multimodal=False\n",
    "train_visual_module=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, \n",
    "                    model, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    loss,\n",
    "                    FNAME):\n",
    "    \n",
    "    today = datetime.date.today()\n",
    "    PATH = f'./checkpoints/{today.strftime(\"checkpoint-%m-%d-%Y\")}/'\n",
    "    if not os.path.exists(PATH):\n",
    "        os.makedirs(PATH)\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler !=None else None,\n",
    "            'loss': loss,\n",
    "            }, f'{PATH}{FNAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_latest_checkpoint(PATH, model, optimizer, scheduler):\n",
    "    checkpoint = torch.load(PATH)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    return epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "    \n",
    "\n",
    "def get_dataset(tokenizer, path=LOAD_PATH, num_data = None, maxlen=64):\n",
    "    df = pd.read_csv(path + 'ml_stacked_data.csv')\n",
    "    if num_data != None:\n",
    "        text_df = df[:num_data]\n",
    "    else:\n",
    "        text_df = df\n",
    "    dataset = ImageCaptionDataset(text_df, tokenizer, maxlen=maxlen)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def train(ml_model, \n",
    "          epochs, \n",
    "          train_data, \n",
    "          val_data, \n",
    "          loss_fct, \n",
    "          optimizer,\n",
    "          scheduler = None,\n",
    "          clip=2.0):\n",
    "    epoch_train_losses = []\n",
    "    epoch_val_losses = []\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        ml_model.train()\n",
    "        epoch_train_loss, num_train_steps = 0, 0\n",
    "        for i, batch in enumerate(tqdm(train_data)):\n",
    "            text, img, target = batch['input_ids'].to(device), batch['image'].to(device), batch['label_ids'].to(device)\n",
    "            #print(img.size())\n",
    "            output = ml_model(text, img)\n",
    "            ml_model.zero_grad()\n",
    "            loss = torch.nn.functional.cross_entropy(output.view(-1, 320001), target.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(ml_model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            num_train_steps += 1\n",
    "            \n",
    "            if i % 2000 == 1:\n",
    "                print(\"Current train loss:\", epoch_train_loss/num_train_steps)\n",
    "\n",
    "        current_train_loss = epoch_train_loss/len(train_data)\n",
    "        epoch_train_losses.append(current_train_loss)\n",
    "        \n",
    "        ml_model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        for i, batch in enumerate(tqdm(val_data)):\n",
    "            text, img, target = batch['input_ids'].to(device), batch['image'].to(device), batch['label_ids'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = ml_model(text, img)\n",
    "                loss = torch.nn.functional.cross_entropy(output.view(-1, output.size(-1)), target.view(-1))\n",
    "            epoch_val_loss += loss.item()\n",
    "            \n",
    "        current_val_loss = epoch_val_loss/len(val_data)\n",
    "        epoch_val_losses.append(current_val_loss)\n",
    "        perplexity = math.exp\n",
    "        print(\"=> Saving checkpoint...\")\n",
    "        if is_multimodal:\n",
    "            if train_visual_module:\n",
    "                FNAME = f'finetuned_visual_multimodal_lstm_{num_data}_{lr}_{epoch}-{current_val_loss:.2f}'\n",
    "                save_checkpoint(epoch, \n",
    "                                ml_model, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                current_val_loss,\n",
    "                                FNAME)\n",
    "            else:\n",
    "                FNAME = f'multimodal_lstm_{num_data}_{lr}_{epoch}-{current_val_loss:.2f}'\n",
    "                save_checkpoint(epoch, \n",
    "                                ml_model, \n",
    "                                optimizer, \n",
    "                                scheduler,\n",
    "                                current_val_loss,\n",
    "                                FNAME)\n",
    "\n",
    "        else:\n",
    "            FNAME = f'benchmark_model_{num_data}_{lr}_{epoch}-{current_val_loss:.2f}'\n",
    "            save_checkpoint(epoch, \n",
    "                            ml_model, \n",
    "                            optimizer, \n",
    "                            scheduler,\n",
    "                            current_val_loss,\n",
    "                            FNAME)\n",
    "        \n",
    "        print(f\"Epoch {epoch}:\\nTrain Loss: {current_train_loss}\\nVal loss: {current_val_loss}\")\n",
    "        \n",
    "    return epoch_train_losses, epoch_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_benchmark(ml_model, \n",
    "                    epochs, \n",
    "                    train_data, \n",
    "                    val_data, \n",
    "                    loss_fct, \n",
    "                    optimizer,\n",
    "                    scheduler=None,\n",
    "                    clip=1.0):\n",
    "    epoch_train_losses = []\n",
    "    epoch_val_losses = []\n",
    "    perplexities = []\n",
    "    for epoch in range(epochs):\n",
    "        print(optimizer.param_groups[0][\"lr\"])\n",
    "        ml_model.train()\n",
    "        epoch_train_loss, num_train_steps = 0, 0\n",
    "        for i, batch in enumerate(tqdm(train_data)):\n",
    "            text, img, target = batch['input_ids'].to(device), batch['image'].to(device), batch['label_ids'].to(device)\n",
    "            output = ml_model(text)\n",
    "            ml_model.zero_grad()\n",
    "            loss = loss_fct(output.view(-1, 320001), target.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(ml_model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            num_train_steps += 1\n",
    "            \n",
    "            if i % 2000 == 1:\n",
    "                print(\"Current train loss:\", epoch_train_loss/num_train_steps)\n",
    "        current_train_loss = epoch_train_loss/len(train_data)\n",
    "        epoch_train_losses.append(current_train_loss)\n",
    "        \n",
    "        ml_model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        for i, batch in enumerate(tqdm(val_data)):\n",
    "            text, img, target = batch['input_ids'].to(device), batch['image'].to(device), batch['label_ids'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = ml_model(text)\n",
    "                loss = loss_fct(output.view(-1, output.size(-1)), target.view(-1))\n",
    "               \n",
    "            epoch_val_loss += loss.item()\n",
    "            \n",
    "        current_val_loss = epoch_val_loss/len(val_data)\n",
    "        epoch_val_losses.append(current_val_loss)\n",
    "        perplexity = math.exp(current_val_loss)\n",
    "        \n",
    "        if len(perplexities) == 0:\n",
    "            perplexities.append(perplexity)\n",
    "        else:\n",
    "            if perplexity >= (3 * perplexities[-1]):\n",
    "                for g in torch.optim.param_groups:\n",
    "                    g['lr'] = g['lr']/2\n",
    "            perplexities.append(perplexity)\n",
    "            \n",
    "        print(f\"Epoch {epoch}:\\nTrain Loss: {current_train_loss}\\nVal loss: {current_val_loss}\")\n",
    "\n",
    "#         print(\"=> Saving checkpoint...\")\n",
    "#         if is_multimodal:\n",
    "#             if train_visual_module:\n",
    "#                 FNAME = f'finetuned_visual_multimodal_lstm_{num_data}_{lr}_{epoch}-{current_val_loss:.2f}'\n",
    "#                 save_checkpoint(epoch, \n",
    "#                                 ml_model, \n",
    "#                                 optimizer, \n",
    "#                                 scheduler,\n",
    "#                                 current_val_loss,\n",
    "#                                 FNAME)\n",
    "#             else:\n",
    "#                 FNAME = f'multimodal_lstm_{num_data}_{lr}_{epoch}-{current_val_loss:.2f}'\n",
    "#                 save_checkpoint(epoch, \n",
    "#                                 ml_model, \n",
    "#                                 optimizer, \n",
    "#                                 scheduler,\n",
    "#                                 current_val_loss,\n",
    "#                                 FNAME)\n",
    "\n",
    "#         else:\n",
    "#             FNAME = f'benchmark_model_{num_data}_{lr}_{epoch}-{current_val_loss:.2f}'\n",
    "#             save_checkpoint(epoch, \n",
    "#                             ml_model, \n",
    "#                             optimizer, \n",
    "#                             scheduler,\n",
    "#                             current_val_loss,\n",
    "#                             FNAME)\n",
    "    return epoch_train_losses, epoch_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_bpe = MultiBPE()\n",
    "all_data = get_dataset(tokenizer=multi_bpe, num_data=num_data,maxlen=maxlen)\n",
    "\n",
    "train_size = int(train_pct * len(all_data))\n",
    "test_size = len(all_data) - train_size\n",
    "train_dataset, val_test_dataset = torch.utils.data.random_split(all_data, [train_size, test_size])\n",
    "\n",
    "train_size = int(0.5 * len(val_test_dataset))\n",
    "test_size = len(val_test_dataset) - train_size\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(val_test_dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "train_data = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "val_data = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "test_data = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "benchmark_model = BenchmarkLSTM().to(device)\n",
    "# visual_multimodal_lstm = MMLSTM(is_multimodal=is_multimodal, \n",
    "#                                       train_visual_module=train_visual_module).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(benchmark_model.parameters(), lr=lr)\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=320000, reduction='mean')\n",
    "\n",
    "# num_warmup_steps = (len(train_dataset) // bs)\n",
    "# num_training_steps = (len(train_dataset) // bs) * epochs\n",
    "# scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "#                                                          num_warmup_steps=num_warmup_steps, \n",
    "#                                                          num_training_steps=num_training_steps)\n",
    "# scheduler = transformers.get_constant_schedule_with_warmup(optimizer, \n",
    "#                                                          num_warmup_steps=num_warmup_steps) \n",
    "train_benchmark(benchmark_model, \n",
    "                epochs, \n",
    "                train_data, \n",
    "                val_data, \n",
    "                loss_fct, \n",
    "                optimizer,\n",
    "                scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ml_model, \n",
    "          epochs, \n",
    "          val_data, \n",
    "          loss_fct, \n",
    "          optimizer,\n",
    "          scheduler = None,\n",
    "          clip=2.0):\n",
    "    \n",
    "    ml_model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    for i, batch in enumerate(tqdm(val_data)):\n",
    "        text, img, target = batch['input_ids'].to(device), batch['image'].to(device), batch['label_ids'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = ml_model(text)\n",
    "            loss = loss_fct(output.view(-1, output.size(-1)), target.view(-1))\n",
    "\n",
    "        epoch_val_loss += loss.item()\n",
    "\n",
    "    current_val_loss = epoch_val_loss/len(val_data)\n",
    "    perplexity = math.exp(current_val_loss)\n",
    "\n",
    "\n",
    "    print(f\"Val loss: {current_val_loss}\")\n",
    "    print(f\"Val perp: {perplexity}\")\n",
    "\n",
    "    return current_val_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(benchmark_model, \n",
    "    epochs, \n",
    "    test_data, \n",
    "    loss_fct, \n",
    "    optimizer,\n",
    "    scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_multimodal:\n",
    "    if train_visual_module:\n",
    "        torch.save(visual_multimodal_lstm.state_dict(), \n",
    "                   f'./saved_models/finetuned_visual_multimodal_lstm_{num_data}_{lr}_{epochs}v4')\n",
    "    else:\n",
    "        torch.save(train_visual_multimodal_lstm.state_dict(), \n",
    "                   f'./saved_models/multimodal_lstm_{num_data}_{lr}_{epochs}v4')\n",
    "else:\n",
    "    torch.save(benchmark_model.state_dict(), \n",
    "               f'./saved_models/benchmark_model_{num_data}_{lr}_{epochs}v7')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
