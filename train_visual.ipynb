{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nkn002/.local/lib/python3.9/site-packages/torch/cuda/__init__.py:83: UserWarning: CUDA initialization: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW (Triggered internally at  ../c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import PIL\n",
    "import math\n",
    "import datetime\n",
    "import os\n",
    "from bpemb import BPEmb\n",
    "from torch import nn\n",
    "from image_caption_dataset import ImageCaptionDataset\n",
    "from multi_bpe import MultiBPE\n",
    "from multimodal_model import MMLSTM, BenchmarkLSTM\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LOAD_PATH = './data/'\n",
    "seed = 420\n",
    "num_data = 200000\n",
    "maxlen = 32\n",
    "epochs = 15\n",
    "train_pct = 0.8\n",
    "bs=32\n",
    "lr = 1.0/math.sqrt(32/bs)\n",
    "is_multimodal=True\n",
    "train_visual_module=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "# torch.cuda.manual_seed(seed)\n",
    "# torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "np.random.seed(seed)  # Numpy module.\n",
    "random.seed(seed)  # Python random module.\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, \n",
    "                    model, \n",
    "                    optimizer, \n",
    "                    scheduler,\n",
    "                    loss,\n",
    "                    FNAME):\n",
    "    \n",
    "    today = datetime.date.today()\n",
    "    PATH = f'./checkpoints/{today.strftime(\"checkpoint-%m-%d-%Y\")}/'\n",
    "    if not os.path.exists(PATH):\n",
    "        os.makedirs(PATH)\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict() if scheduler !=None else None,\n",
    "            'loss': loss,\n",
    "            }, f'{PATH}{FNAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(LOAD_PATH + 'ml_stacked_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l = {}\n",
    "for caption in df.caption:\n",
    "    length = len(caption.split())\n",
    "    if length in l:\n",
    "        l[length] += 1  \n",
    "    else:\n",
    "        l[length] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_fn(worker_id):\n",
    "    np.random.seed(int(seed))\n",
    "    \n",
    "def process_img(image_path): \n",
    "    transform = torchvision.transforms.Compose([\n",
    "        # Resize image to 224 x 224 as required by most vision models\n",
    "        torchvision.transforms.Resize(\n",
    "            size=(224, 224)\n",
    "        ),\n",
    "        # Convert PIL image to tensor with image values in [0, 1]\n",
    "        torchvision.transforms.ToTensor(),\n",
    "\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    im = PIL.Image.open(image_path)\n",
    "    image = im.convert('RGB')\n",
    "    image = transform(image)\n",
    "    \n",
    "    return image.view(1, image.size(0), image.size(1), image.size(2))\n",
    "\n",
    "def get_dataset(tokenizer, path=LOAD_PATH, num_data = None, maxlen=64):\n",
    "    df = pd.read_csv(path + 'ml_stacked_data.csv')    \n",
    "    if num_data != None:\n",
    "        text_df = df[:num_data]\n",
    "    else:\n",
    "        text_df = df\n",
    "    dataset = ImageCaptionDataset(text_df, tokenizer, maxlen=maxlen)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def train(ml_model, \n",
    "          epochs, \n",
    "          train_data, \n",
    "          val_data, \n",
    "          loss_fct, \n",
    "          optimizer,\n",
    "          scheduler = None,\n",
    "          clip=2.0):\n",
    "    epoch_train_losses = []\n",
    "    epoch_val_losses = []\n",
    "    perplexities = []\n",
    "    for epoch in range(epochs):\n",
    "        print(optimizer.param_groups[0][\"lr\"])\n",
    "        ml_model.train()\n",
    "        epoch_train_loss, num_train_steps = 0, 0\n",
    "        for i, batch in enumerate(tqdm(train_data)):\n",
    "            text, img, target = batch['input_ids'].to(device), batch['image'].to(device), batch['label_ids'].to(device)\n",
    "            output = ml_model(text, img)\n",
    "            ml_model.zero_grad()\n",
    "            loss = loss_fct(output.view(-1, 320001), target.view(-1))\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(ml_model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            num_train_steps += 1\n",
    "            \n",
    "            if i % 2000 == 1:\n",
    "                print(\"Current train loss:\", epoch_train_loss/num_train_steps)\n",
    "        current_train_loss = epoch_train_loss/len(train_data)\n",
    "        epoch_train_losses.append(current_train_loss)\n",
    "        \n",
    "        ml_model.eval()\n",
    "        epoch_val_loss = 0\n",
    "        for i, batch in enumerate(tqdm(val_data)):\n",
    "            text, img, target = batch['input_ids'].to(device), batch['image'].to(device), batch['label_ids'].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = ml_model(text, img)\n",
    "                loss = loss_fct(output.view(-1, output.size(-1)), target.view(-1))\n",
    "               \n",
    "            epoch_val_loss += loss.item()\n",
    "            \n",
    "        current_val_loss = epoch_val_loss/len(val_data)\n",
    "        epoch_val_losses.append(current_val_loss)\n",
    "        perplexity = math.exp(current_val_loss)\n",
    "        \n",
    "        if len(perplexities) == 0:\n",
    "            perplexities.append(perplexity)\n",
    "        else:\n",
    "            if perplexity >= (3 * perplexities[-1]):\n",
    "                for g in torch.optim.param_groups:\n",
    "                    g['lr'] = g['lr']/2\n",
    "            perplexities.append(perplexity)\n",
    "            \n",
    "        print(f\"Epoch {epoch}:\\nTrain Loss: {current_train_loss}\\nVal loss: {current_val_loss}\")\n",
    "\n",
    "#         print(\"=> Saving checkpoint...\")\n",
    "#         if is_multimodal:\n",
    "#             if train_visual_module:\n",
    "#                 FNAME = f'finetuned_visual_multimodal_lstm_{num_data}_{lr}_{epoch}-{current_val_loss:.2f}'\n",
    "#                 save_checkpoint(epoch, \n",
    "#                                 ml_model, \n",
    "#                                 optimizer, \n",
    "#                                 scheduler,\n",
    "#                                 current_val_loss,\n",
    "#                                 FNAME)\n",
    "#             else:\n",
    "#                 FNAME = f'multimodal_lstm_{num_data}_{lr}_{epoch}-{current_val_loss:.2f}'\n",
    "#                 save_checkpoint(epoch, \n",
    "#                                 ml_model, \n",
    "#                                 optimizer, \n",
    "#                                 scheduler,\n",
    "#                                 current_val_loss,\n",
    "#                                 FNAME)\n",
    "\n",
    "#         else:\n",
    "#             FNAME = f'benchmark_model_{num_data}_{lr}_{epoch}-{current_val_loss:.2f}'\n",
    "#             save_checkpoint(epoch, \n",
    "#                             ml_model, \n",
    "#                             optimizer, \n",
    "#                             scheduler,\n",
    "#                             current_val_loss,\n",
    "#                             FNAME)\n",
    "    return epoch_train_losses, epoch_val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[Errno 2] No such file or directory: './images/000000046310.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m loss_fct \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m320000\u001b[39m, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# num_warmup_steps = (len(train_dataset) // bs)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# num_training_steps = (len(train_dataset) // bs) * epochs\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# scheduler = transformers.get_constant_schedule_with_warmup(optimizer, \u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#                                                          num_warmup_steps=num_warmup_steps) \u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_multimodal_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(ml_model, epochs, train_data, val_data, loss_fct, optimizer, scheduler, clip)\u001b[0m\n\u001b[1;32m     48\u001b[0m ml_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     49\u001b[0m epoch_train_loss, num_train_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_data)):\n\u001b[1;32m     51\u001b[0m     text, img, target \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m     output \u001b[38;5;241m=\u001b[39m ml_model(text, img)\n",
      "File \u001b[0;32m/usr/remote/apps/anaconda/2021.11-deeplearn/lib/python3.9/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    691\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 692\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataset.py:290\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/projects/torch_multimodal_bilingual_lstm/image_caption_dataset.py:38\u001b[0m, in \u001b[0;36mImageCaptionDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     36\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m encoded[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     37\u001b[0m label_ids \u001b[38;5;241m=\u001b[39m encoded[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m---> 38\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageReadMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRGB\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# image = im.convert('RGB')\u001b[39;00m\n\u001b[1;32m     40\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(im)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/io/image.py:251\u001b[0m, in \u001b[0;36mread_image\u001b[0;34m(path, mode)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    250\u001b[0m     _log_api_usage_once(read_image)\n\u001b[0;32m--> 251\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torchvision/io/image.py:47\u001b[0m, in \u001b[0;36mread_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m     46\u001b[0m     _log_api_usage_once(read_file)\n\u001b[0;32m---> 47\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/_ops.py:143\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [Errno 2] No such file or directory: './images/000000046310.jpg'"
     ]
    }
   ],
   "source": [
    "multi_bpe = MultiBPE()\n",
    "all_data = get_dataset(tokenizer=multi_bpe, num_data=num_data,maxlen=maxlen)\n",
    "\n",
    "train_size = int(train_pct * len(all_data))\n",
    "test_size = len(all_data) - train_size\n",
    "train_dataset, val_test_dataset = torch.utils.data.random_split(all_data, [train_size, test_size])\n",
    "\n",
    "train_size = int(0.5 * len(val_test_dataset))\n",
    "test_size = len(val_test_dataset) - train_size\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(val_test_dataset, [train_size, test_size])\n",
    "\n",
    "\n",
    "train_data = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "val_data = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
    "test_data = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n",
    "\n",
    "# benchmark_model = BenchmarkLSTM().to(device)\n",
    "visual_multimodal_lstm = MMLSTM(is_multimodal=is_multimodal, \n",
    "                                      train_visual_module=train_visual_module).to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(visual_multimodal_lstm.parameters(), lr=lr)\n",
    "loss_fct = torch.nn.CrossEntropyLoss(ignore_index=320000, reduction='mean')\n",
    "\n",
    "# num_warmup_steps = (len(train_dataset) // bs)\n",
    "# num_training_steps = (len(train_dataset) // bs) * epochs\n",
    "# scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
    "#                                                          num_warmup_steps=num_warmup_steps, \n",
    "#                                                          num_training_steps=num_training_steps)\n",
    "# scheduler = transformers.get_constant_schedule_with_warmup(optimizer, \n",
    "#                                                          num_warmup_steps=num_warmup_steps) \n",
    "train(visual_multimodal_lstm, \n",
    "    epochs, \n",
    "    train_data, \n",
    "    val_data, \n",
    "    loss_fct, \n",
    "    optimizer,\n",
    "    scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_multimodal:\n",
    "    if train_visual_module:\n",
    "        torch.save(visual_multimodal_lstm.state_dict(), \n",
    "                   f'./saved_models/finetuned_visual_multimodal_lstm_{num_data}_{lr}_{epochs}v5')\n",
    "    else:\n",
    "        torch.save(visual_multimodal_lstm.state_dict(), \n",
    "                   f'./saved_models/multimodal_lstm_{num_data}_{lr}_{epochs}v6')\n",
    "else:\n",
    "    torch.save(benchmark_model.state_dict(), \n",
    "               f'./saved_models/benchmark_model_{num_data}_{lr}_{epochs}v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(ml_model, \n",
    "          epochs, \n",
    "          val_data, \n",
    "          loss_fct, \n",
    "          optimizer,\n",
    "          scheduler = None,\n",
    "          clip=2.0):\n",
    "    \n",
    "    ml_model.eval()\n",
    "    epoch_val_loss = 0\n",
    "    for i, batch in enumerate(tqdm(val_data)):\n",
    "        text, img, target = batch['input_ids'].to(device), batch['image'].to(device), batch['label_ids'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = ml_model(text, img)\n",
    "            loss = loss_fct(output.view(-1, output.size(-1)), target.view(-1))\n",
    "\n",
    "        epoch_val_loss += loss.item()\n",
    "\n",
    "    current_val_loss = epoch_val_loss/len(val_data)\n",
    "    perplexity = math.exp(current_val_loss)\n",
    "\n",
    "\n",
    "    print(f\"Val loss: {current_val_loss}\")\n",
    "    print(f\"Val perp: {perplexity}\")\n",
    "\n",
    "    return current_val_loss, perplexity\n",
    "\n",
    "test(visual_multimodal_lstm, \n",
    "    epochs, \n",
    "    test_data, \n",
    "    loss_fct, \n",
    "    optimizer,\n",
    "    scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
